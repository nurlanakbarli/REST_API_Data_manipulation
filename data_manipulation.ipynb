{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import urllib \n",
    "import subprocess\n",
    "import os\n",
    "import datetime\n",
    "from datetime import date\n",
    "from random import randint\n",
    "from sqlite3 import Date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from itertools import cycle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare the information needed to access the server\n",
    "\n",
    "account='your login'\n",
    "passwd='your password?'\n",
    "URL = 'your url'\n",
    "ping = requests.get(URL+'/ping')\n",
    "if ping.status_code == 200:\n",
    "    try:\n",
    "        data = ping.json()\n",
    "        print(data)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"the request isn'it in the json format.\")\n",
    "else:\n",
    "    print(f\"request error with : {ping.status_code}.\")\n",
    "\n",
    "\n",
    "login= requests.post(URL+'/login', json={\"identifier\":account,\"password\":passwd})\n",
    "token=login.json()['token']\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To see all tables attached to a Data Modul.\n",
    "### In our systeme a Data Modul can contain severel tables, and we have teh REST API to check the Data Moduls. This API is Get/modul/modul_name/definition \n",
    "### In the place of sending this API by Insomnia or Postman we will use 'request' library of Python in this script.\n",
    "\n",
    "\n",
    "module_name = input(\"Veuillez entrer le nom du module : \")\n",
    "module_definition = requests.get(f'{URL}/module/{module_name}/definition',  headers={'Authorization': f'{token}'})\n",
    "if module_definition.status_code == 200 :\n",
    "    fact_table = module_definition.json().get('tables')\n",
    "    print(f'fact_table : {fact_table}')    \n",
    "        \n",
    "    zips = []\n",
    "    x = module_definition.json().get('arrays')\n",
    "    for i in range(len(x)):\n",
    "        zip = x[i].get('zipTable', {})  \n",
    "        zips.append(zip)\n",
    "    print ( f\"zips : {zips}\" )\n",
    "        \n",
    "    dictionaries = module_definition.json().get('dictionaries', {})\n",
    "    if dictionaries:\n",
    "       dico = set(dictionaries.values())\n",
    "       print(f'dico : {dico}')\n",
    "    else :\n",
    "       dico = []\n",
    "       print(f'No dico')\n",
    "       \n",
    "    facet = module_definition.json().get('facets', {})\n",
    "    print(f'facet : {facet}')\n",
    "\n",
    "       \n",
    "else:\n",
    "    print(f\"ERROR\")   \n",
    "    \n",
    "\n",
    "#ingestion_tables = zips + list(dico) + fact_table\n",
    "ingestion_tables = list(dico) + fact_table\n",
    "zip_dico = zips + list(dico)  \n",
    "print(f'nb_table de ccr_ingestion_tables: {len(ingestion_tables)} : {ingestion_tables}')\n",
    "print(zip_dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we will find the columns and theirs count for each table \n",
    "\n",
    "for table_name in ingestion_tables:\n",
    "    ingest_settings = requests.get(f'{URL}/module/{module_name}/tables/{table_name}/ingestion_settings', headers={'Authorization': f'{token}'})\n",
    "    if ingest_settings.status_code == 200:\n",
    "        insertquery = ingest_settings.json().get('insertQuery')\n",
    "        match = re.search(r\"input\\('(.+?)'\\)\", insertquery)\n",
    "        my_string = match.group(1)\n",
    "    else:\n",
    "        print(f\"ERROR\")\n",
    "\n",
    "    fields = []\n",
    "    for elemt in my_string.split(','):\n",
    "        elemt = elemt.strip('\\'').lstrip(' ')\n",
    "        fields.append(elemt.split(\" \"))\n",
    "\n",
    "    variable_types_count = {}  \n",
    "    total_occurrences = 0  \n",
    "\n",
    "    column_names = [] \n",
    "\n",
    "    for elmt in fields:\n",
    "        column_name = elmt[0]  \n",
    "        column_type = elmt[1]  \n",
    "\n",
    "        variable_types_count[column_type] = variable_types_count.get(column_type, 0) + 1\n",
    "        total_occurrences += 1\n",
    "\n",
    "        column_names.append(column_name)\n",
    "\n",
    "    print(f'In the table: {table_name}')\n",
    "    print(\"Columns'name:\")\n",
    "    for column_name in column_names:\n",
    "        print(column_name)\n",
    "    for v_type, count in variable_types_count.items():\n",
    "        \n",
    "        print(f\"{v_type}: {count}\")\n",
    "    print(f\"column count: {total_occurrences}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will check the joins between the fact table and its dictionnaries\n",
    "\n",
    "datamodel = requests.get(f'{URL}/datamodel/{module_name}',  headers={'Authorization': f'{token}'})\n",
    "joins = datamodel.json().get('rawDataModel').get('joins')  #rwdatamodel denn sonra {} geldiyi ucun day [0] yazmiriq, [] gelse idi yazardiq\n",
    "factkey = []\n",
    "dicokey=[]\n",
    "for i in range(len(joins)) :\n",
    "    factkey_value = joins[i].get('key1').get('key')[0]\n",
    "    factkey.append(factkey_value)\n",
    "    dicokey_value = joins[i].get('key2').get('key')[0]\n",
    "    dicokey.append(dicokey_value)\n",
    "print(factkey)\n",
    "print(dicokey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the final step of script and also is crucial step as the main idea is to generate the data and ingest them into data base.\n",
    "\n",
    "for table_name in ingestion_tables :\n",
    "    ingest_settings = requests.get(f'{URL}/module/{module_name}/tables/{table_name}/ingestion_settings',  headers={'Authorization': f'{token}'})\n",
    "    # here we will get the body of each table: I mean, we will have tha list of columns and theirs clickhouse types (for example, column_1 (String), column_2(float64) etc.)\n",
    "    if ingest_settings.status_code == 200:\n",
    "        insertquery = ingest_settings.json().get('insertQuery')\n",
    "        match = re.search(r\"input\\('(.+?)'\\)\", insertquery)\n",
    "        my_string= match.group(1)\n",
    "    else:\n",
    "        print(f\"ERROR OF INSERTQUERY\")\n",
    "    \n",
    "    fields = []\n",
    "    for elemt in my_string.split(','):\n",
    "        elemt = elemt.strip('\\'').lstrip(' ')\n",
    "        fields.append(elemt.split(\" \"))\n",
    "\n",
    "    \n",
    "    N = 10  # this is the number of rows for each csv file\n",
    "    startdate=datetime.date(2022,4,20)\n",
    "    datavec=[]\n",
    "    datevec=[str(startdate-datetime.timedelta(days=i)) for i in range(0,500)]\n",
    "    datavec=[random.sample(datevec, 250) for i in range(2)]\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    \n",
    "    output_folder = module_name\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    #once we have all info about the body of tables, we will generate the data based on this info by runnig this for step:\n",
    "    \n",
    "    for itt in range(3,5): #btw, ths is for  Date, we will use it in the Rest API which is for ingestion. Actually, for each ingestion I'll use different dates. So, I'll ingest the data two times for each time. 1e is for the date 03, and second is for the 04 \n",
    "        for elmt in fields[:]:\n",
    "            if('String' in elmt[1] and 'Array' not in elmt[1]):\n",
    "                df[elmt[0]] = [elmt[0]+str(random.randint(3000,5000)) for i in range(N)]\n",
    "                df[elmt[0]] = ''+ df[elmt[0]] + 'berlin'\n",
    "            if('String' in elmt[1] and 'Array'  in elmt[1]):\n",
    "                df[elmt[0]] = [[elmt[0]+str(random.randint(3000,5000)) for i in range(3)] for i in range(N)]\n",
    "            \n",
    "            if ('String' not in elmt[1] and 'Array' in elmt[1]):\n",
    "                if ('Float' in elmt[1]):\n",
    "                    df[elmt[0]] = np.random.uniform(-399.5,399.5,(N,3)).tolist()\n",
    "                if('Int8' in elmt[1]):\n",
    "                    df[elmt[0]] = np.random.randint(-100,100,(N,3)).tolist()\n",
    "                if('Int32' in elmt[1]):\n",
    "                    df[elmt[0]] = np.random.randint(-10000,10000,(N,3)).tolist()\n",
    "                if('Int64' in elmt[1]):\n",
    "                    df[elmt[0]] = np.random.randint(-100000,100000,(N,3)).tolist()\n",
    "                if('UInt8' in elmt[1]):\n",
    "                    df[elmt[0]] = np.random.randint(0,100,(N,3)).tolist() \n",
    "                if('UInt64' in elmt[1]):\n",
    "                    df[elmt[0]] = np.random.randint(0,1000000,(N,3)).tolist()\n",
    "                if('UInt32' in elmt[1]):\n",
    "                    df[elmt[0]] = np.random.randint(0,100000,(N,3)).tolist()                    \n",
    "                if 'Nullable(Int)' in elmt[1]:\n",
    "                    df[elmt[0]] = ''        \n",
    "                if 'Nullable(Float)' in elmt[1]:\n",
    "                    df[elmt[0]] = '' \n",
    "                    \n",
    "            if ('String' not in elmt[1] and 'Array' not in elmt[1]):\n",
    "                if('Float' in elmt[1]):\n",
    "                    df[elmt[0]] = np.random.uniform(-399.5,399.5,N).tolist()\n",
    "                if('Int8' in elmt[1]):\n",
    "                    df[elmt[0]] = np.random.randint(-100,100,N).tolist()\n",
    "                if('Int32' in elmt[1]):\n",
    "                    df[elmt[0]] = np.random.randint(-10000,10000,N).tolist()\n",
    "                if('Int64' in elmt[1]):\n",
    "                    df[elmt[0]] = np.random.randint(-100000,100000,N).tolist()\n",
    "                if('UInt8' in elmt[1]):\n",
    "                    df[elmt[0]] = np.random.randint(0,10000,N).tolist() \n",
    "                if('UInt64' in elmt[1]):\n",
    "                    df[elmt[0]] = np.random.randint(0,100000,N).tolist()\n",
    "                if('UInt32' in elmt[1]):\n",
    "                    df[elmt[0]] = np.random.randint(0,1000000,N).tolist()                    \n",
    "                if 'Nullable(Int)' in elmt[1]:\n",
    "                    df[elmt[0]] = ''        \n",
    "                if 'Nullable(Float)' in elmt[1]:\n",
    "                    df[elmt[0]] = ''       \n",
    "            \n",
    "        \n",
    "            if('Date' in elmt[1] and 'DateTime' not in elmt[1]):\n",
    "                df[elmt[0]] = [date.today() for i in range(N)]\n",
    "            if('DateTime' in elmt[1]):\n",
    "                df[elmt[0]] = [datetime.datetime.now() for i in range(N)]        \n",
    "        join_1 = cycle(['L1composite', 'L2composite', 'L3composite', 'L4composite', 'L5composite'])\n",
    "        join_2= cycle(['R1composite', 'R2composite', 'R3composite', 'R4composite', 'R5composite'])\n",
    "        join_3 = cycle(['B1composite', 'B2composite', 'B3composite', 'B4composite', 'B5composite'])\n",
    "        join_4 = cycle(['111', '112', '113', '114', '115'])\n",
    "        join_5 = cycle(['511', '521', '531', '541', '551'])\n",
    "        \n",
    "        if 'join_1' in df:\n",
    "            df['join_1'] = [next(join_1) for _ in range(len(df))]\n",
    "        if 'join_2' in df:\n",
    "            df['join_2'] = [next(join_2) for _ in range(len(df))]\n",
    "        if 'join_3' in df:\n",
    "            df['join_3'] = [next(join_3) for _ in range(len(df))]\n",
    "        if 'join_4' in df :\n",
    "            df['join_4'] = [next(join_4) for _ in range(len(df))]\n",
    "        if 'join_5' in df :\n",
    "            df['join_5'] = [next(join_5) for _ in range(len(df))]\n",
    "            \n",
    "        csv_path = os.path.join(output_folder, f\"{table_name}_{itt}.csv\")\n",
    "        df.to_csv(csv_path, header=True, index=False)\n",
    "        print(f'{table_name}_{itt}.csv a été créé')\n",
    "\n",
    "        # After preparing the data, we will start ingestion by the Rest API \n",
    "        # before we will ingest the data into zip and dictionnary tables:        \n",
    "        if table_name in zip_dico :\n",
    "               \n",
    "                dico_ingestion = f'{URL}/tables/{table_name}/2024-04-{itt:02}' ### :02 just pour avoir les dates comme 01, 02 etc\n",
    "                headers = {\n",
    "                'Authorization': f'{token}',\n",
    "                'Content-Type': 'text/csv;header=present'\n",
    "                    }\n",
    "\n",
    "                with open(csv_path, 'rb') as file:\n",
    "                    data = file.read()\n",
    "\n",
    "                response = requests.post(dico_ingestion, headers=headers, data=data, verify=False)\n",
    "    \n",
    "                if response.status_code == 204:\n",
    "                    print(f\"for {table_name} le fichier {table_name}_{itt} a été ingeré\")\n",
    "                else:\n",
    "                    print(f\"Error in {table_name}: {response.status_code}, {response.text}\")\n",
    "\n",
    "        # and in the final part the fact table(main table) will be ingested\n",
    "\n",
    "        elif table_name in fact_table:\n",
    "            transactionId = requests.post(f'{URL}/tables/{table_name}/transaction/2024-04-{itt:02}/from/official/ingestion',  headers={'Authorization': f'{token}'})\n",
    "            transactionId= transactionId.content.decode('utf-8')\n",
    "            Fact_table_ingest = f'{URL}/tables/{table_name}/commits/{transactionId}/2024-04-{itt:02}' ### :02 just pour avoir les dates comme 01, 02 etc\n",
    "            headers = {\n",
    "                'Authorization': f'{token}',\n",
    "                'Content-Type': 'text/csv;header=present'\n",
    "                }\n",
    "\n",
    "            with open(csv_path, 'rb') as file:\n",
    "                data = file.read()\n",
    "\n",
    "            response = requests.post(Fact_table_ingest, headers=headers, data=data, verify=False)\n",
    "            if response.status_code == 200:\n",
    "                closetransaction = f\"{URL}/tables/{table_name}/closeTransaction/{transactionId}/2024-04-{itt:02}/to/official\"\n",
    "\n",
    "                headers = {\n",
    "            \"Authorization\": token\n",
    "                 }\n",
    "\n",
    "                response = requests.post(closetransaction, headers=headers, verify=False)\n",
    "                print(f\"for {table_name} le fichier {table_name}_{itt} a été ingeré : {response}\")\n",
    "    \n",
    "            else:\n",
    "                print(f\"Error for {table_name} : {response.status_code}, {response.text}\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
